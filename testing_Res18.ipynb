{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Linear Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f20348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurations ---\n",
    "# Set the device to CUDA if available, otherwise use the CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Root directory of the EuroSAT dataset\n",
    "DATA_ROOT = \"data/eurosat/eurosat/2750\"\n",
    "# Path to the pretrained backbone checkpoint\n",
    "BACKBONE_CKPT = \"models/res18/backbone/backbone_res18.pt\"\n",
    "# Directory to save checkpoints\n",
    "CKPT_DIR = \"runs/linear_probe_eurosat_res18/checkpoints_linear_probe_res18\"\n",
    "# Directory to save TensorBoard logs\n",
    "LOG_DIR = \"runs/linear_probe_eurosat_res18\"\n",
    "# Number of training epochs\n",
    "NUM_EPOCHS = 100\n",
    "# Batch size for training and validation\n",
    "BATCH_SIZE = 256\n",
    "# Learning rate for the optimizer\n",
    "LR = 0.01\n",
    "# Momentum for the SGD optimizer\n",
    "MOMENTUM = 0.9\n",
    "# Weight decay for regularization\n",
    "WEIGHT_DECAY = 1e-4\n",
    "# Number of classes in the EuroSAT dataset\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Class names for the EuroSAT dataset\n",
    "class_names = [\n",
    "    \"AnnualCrop\", \"Forest\", \"HerbaceousVegetation\", \"Highway\",\n",
    "    \"Industrial\", \"Pasture\", \"PermanentCrop\", \"Residential\",\n",
    "    \"River\", \"SeaLake\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce066def",
   "metadata": {},
   "source": [
    "### 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e153145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation for normalization\n",
    "mean = [0.48241806, 0.48080587, 0.47794071]\n",
    "std = [0.19021621, 0.16879530, 0.14623168]\n",
    "# Preprocessing pipeline for the images\n",
    "preproc = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# Custom dataset class for EuroSAT\n",
    "class EuroSATJPG(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.paths = glob.glob(os.path.join(root_dir, \"*\", \"*.jpg\"))\n",
    "        self.paths.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        label = os.path.basename(os.path.dirname(path))\n",
    "        y = class_names.index(label)\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "# Create the dataset and split into training and validation sets\n",
    "ds = EuroSATJPG(DATA_ROOT, transform=preproc)\n",
    "n = len(ds)\n",
    "n_train = int(0.8 * n)\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [n_train, n - n_train])\n",
    "# Create data loaders for training and validation\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Function to save the validation dataset\n",
    "def save_dataset(dataset, filename):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for img, label in dataset:\n",
    "        data.append(img)\n",
    "        labels.append(label)\n",
    "    data_tensor = torch.stack(data)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    torch.save((data_tensor, labels_tensor), filename)\n",
    "\n",
    "# Save the validation dataset\n",
    "save_dataset(val_ds, \"val_dataset.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b9ae8",
   "metadata": {},
   "source": [
    "### 4. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature dimension for ResNet-18\n",
    "feat_dim = 512\n",
    "# Load the ResNet-18 model with pretrained weights\n",
    "encoder = models.resnet18(pretrained=True)\n",
    "# Remove the final classification layer\n",
    "encoder.fc = nn.Identity()\n",
    "# Load the pretrained backbone checkpoint\n",
    "ckpt = torch.load(BACKBONE_CKPT, map_location=DEVICE)\n",
    "# Load the state dict into the encoder\n",
    "state = {k.replace(\"backbone.\", \"\"): v for k, v in ckpt[\"model_state_dict\"].items() if k.startswith(\"backbone.\")}\n",
    "encoder.load_state_dict(state, strict=True)\n",
    "# Move the encoder to the device and set to evaluation mode\n",
    "encoder.to(DEVICE).eval()\n",
    "# Freeze the encoder weights\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Define the linear head for classification\n",
    "head = nn.Linear(feat_dim, NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc363f",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(head.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "# TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "\n",
    "# Projector config for embeddings\n",
    "metadata_path = os.path.join(LOG_DIR, \"metadata.tsv\")\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    f.write(\"Label\\n\")\n",
    "projector_config = f\"\"\"\n",
    "embeddings {{\n",
    "  tensor_name: \"LinearProbe/Embeddings\"\n",
    "  metadata_path: \"{os.path.basename(metadata_path)}\"\n",
    "}}\n",
    "\"\"\"\n",
    "with open(os.path.join(LOG_DIR, \"projector_config.pbtxt\"), \"w\") as f:\n",
    "    f.write(projector_config)\n",
    "\n",
    "# Training and validation loop\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Training\n",
    "    head.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    for x, y in tqdm(train_loader, desc=f\"[Train] Epoch {epoch}/{NUM_EPOCHS}\"):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            feats = encoder(x)\n",
    "        logits = head(feats)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * y.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        all_preds.append(pred.cpu())\n",
    "        all_labels.append(y.cpu())\n",
    "\n",
    "    # Log training metrics\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    scheduler.step()\n",
    "    writer.add_scalar(\"LinearProbe/Train_Loss\", train_loss, epoch)\n",
    "    writer.add_scalar(\"LinearProbe/Train_Acc\", train_acc, epoch)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    writer.add_scalar(\"LinearProbe/Train_Precision\", precision_score(all_labels, all_preds, average='weighted', zero_division=1), epoch)\n",
    "    writer.add_scalar(\"LinearProbe/Train_Recall\", recall_score(all_labels, all_preds, average='weighted', zero_division=1), epoch)\n",
    "    writer.add_scalar(\"LinearProbe/Train_F1\", f1_score(all_labels, all_preds, average='weighted', zero_division=1), epoch)\n",
    "\n",
    "    # Validation\n",
    "    head.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_feats, all_preds_list, all_labels_list, all_imgs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            feats = encoder(x)\n",
    "            logits = head(feats)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "            all_feats.append(feats.cpu())\n",
    "            all_preds_list.append(pred.cpu())\n",
    "            all_labels_list.append(y.cpu())\n",
    "            all_imgs.append(x.cpu())\n",
    "\n",
    "    # Log validation metrics\n",
    "    val_acc = correct / total\n",
    "    writer.add_scalar(\"LinearProbe/Val_Acc\", val_acc, epoch)\n",
    "\n",
    "    val_preds = torch.cat(all_preds_list)\n",
    "    val_labels = torch.cat(all_labels_list)\n",
    "    writer.add_scalar(\"LinearProbe/Val_Precision\", precision_score(val_labels, val_preds, average='weighted', zero_division=1), epoch)\n",
    "    writer.add_scalar(\"LinearProbe/Val_Recall\", recall_score(val_labels, val_preds, average='weighted', zero_division=1), epoch)\n",
    "    writer.add_scalar(\"LinearProbe/Val_F1\", f1_score(val_labels, val_preds, average='weighted', zero_division=1), epoch)\n",
    "\n",
    "    # Log embeddings\n",
    "    feats_cat = torch.cat(all_feats)\n",
    "    labels_cat = torch.cat(all_labels_list)\n",
    "\n",
    "    max_pts = 1000\n",
    "    if feats_cat.size(0) > max_pts:\n",
    "        idx = torch.randperm(feats_cat.size(0))[:max_pts]\n",
    "        feats_sample = feats_cat[idx]\n",
    "        labels_sample = labels_cat[idx].tolist()\n",
    "    else:\n",
    "        feats_sample = feats_cat\n",
    "        labels_sample = labels_cat.tolist()\n",
    "\n",
    "    writer.add_embedding(\n",
    "        mat=feats_sample,\n",
    "        metadata=labels_sample,\n",
    "        global_step=epoch,\n",
    "        tag=\"LinearProbe/Embeddings\"\n",
    "    )\n",
    "\n",
    "    # Log t-SNE plot\n",
    "    tsne = TSNE(n_components=2)\n",
    "    feats_2d = tsne.fit_transform(feats_sample.numpy())\n",
    "    fig_tsne, ax_tsne = plt.subplots()\n",
    "    ax_tsne.scatter(feats_2d[:, 0], feats_2d[:, 1], c=labels_sample, s=5)\n",
    "    ax_tsne.set_title(f\"t-SNE Val (Epoch {epoch})\")\n",
    "    writer.add_figure(\"LinearProbe/tSNE_Val\", fig_tsne, epoch)\n",
    "    plt.close(fig_tsne)\n",
    "\n",
    "    # Log confusion matrix\n",
    "    cm = confusion_matrix(labels_cat.tolist(), val_preds.tolist())\n",
    "    fig_cm, ax_cm = plt.subplots(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                ax=ax_cm)\n",
    "    ax_cm.set_xlabel(\"Predicted\")\n",
    "    ax_cm.set_ylabel(\"True\")\n",
    "    writer.add_figure(\"LinearProbe/ConfusionMatrix_Val\", fig_cm, epoch)\n",
    "    plt.close(fig_cm)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Train: {train_loss:.4f}/{train_acc:.3f} | Val: {val_acc:.3f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        path = os.path.join(CKPT_DIR, f\"best_head_epoch{epoch:03d}.pth\")\n",
    "        torch.save({\"epoch\": epoch, \"head_state_dict\": head.state_dict(), \"val_acc\": val_acc}, path)\n",
    "\n",
    "# Save the final model\n",
    "final_path = os.path.join(CKPT_DIR, \"head_final.pth\")\n",
    "torch.save(head.state_dict(), final_path)\n",
    "print(f\"Final head saved at {final_path}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# --- Configuration ---\n",
    "# Choose GPU if available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Class labels for the EuroSAT dataset\n",
    "class_names = [\n",
    "    \"AnnualCrop\", \"Forest\", \"HerbaceousVegetation\", \"Highway\", \"Industrial\",\n",
    "    \"Pasture\", \"PermanentCrop\", \"Residential\", \"River\", \"SeaLake\"\n",
    "]\n",
    "\n",
    "# --- Utility: Inverse normalization for visualization ---\n",
    "# These mean/std values match the ones used during training\n",
    "mean = torch.tensor([0.48241806, 0.48080587, 0.47794071])\n",
    "std  = torch.tensor([0.19021621, 0.16879530, 0.14623168])\n",
    "\n",
    "def unnormalize(img_tensor):\n",
    "    \"\"\"\n",
    "    Reverses normalization on a tensor and converts it to a PIL Image.\n",
    "    \"\"\"\n",
    "    img = img_tensor.clone()\n",
    "    for c in range(3):\n",
    "        img[c] = img[c] * std[c] + mean[c]              # undo each channel normalization\n",
    "    img = img.mul(255).byte().permute(1, 2, 0).cpu()     # to H×W×C uint8\n",
    "    return Image.fromarray(img.numpy())\n",
    "\n",
    "# --- Preprocessing transform for raw images ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist()),\n",
    "])\n",
    "\n",
    "# --- Load SSL-trained ResNet-18 encoder ---\n",
    "encoder = models.resnet18(pretrained=False)\n",
    "encoder.fc = torch.nn.Identity()  # remove classification head\n",
    "encoder.to(device).eval()\n",
    "\n",
    "# Load the checkpoint and extract only the backbone parameters\n",
    "ckpt_ssl = torch.load(\n",
    "    \"models/res18/backbone/backbone_res18.pt\",\n",
    "    map_location=device\n",
    ")\n",
    "state = {\n",
    "    k.replace(\"backbone.\", \"\"): v\n",
    "    for k, v in ckpt_ssl[\"model_state_dict\"].items()\n",
    "    if k.startswith(\"backbone.\")\n",
    "}\n",
    "encoder.load_state_dict(state, strict=True)\n",
    "\n",
    "# --- Load linear probing head ---\n",
    "feat_dim = 512\n",
    "head = torch.nn.Linear(feat_dim, len(class_names)).to(device).eval()\n",
    "\n",
    "ckpt_lp = torch.load(\n",
    "    \"models/res18/linear_probe/head_final.pth\",\n",
    "    map_location=device\n",
    ")\n",
    "# Support two checkpoint formats\n",
    "if \"head_state_dict\" in ckpt_lp:\n",
    "    head.load_state_dict(ckpt_lp[\"head_state_dict\"])\n",
    "else:\n",
    "    head.weight.data = ckpt_lp[\"weight\"]\n",
    "    head.bias.data   = ckpt_lp[\"bias\"]\n",
    "\n",
    "# --- Load validation dataset ---\n",
    "# The saved file may contain (images_tensor, labels_tensor) or a list of (img,label) pairs\n",
    "val_dataset = torch.load(os.path.expanduser(\"~/val_dataset.pth\"), map_location=device)\n",
    "if isinstance(val_dataset, (list, tuple)) and len(val_dataset) == 2 and isinstance(val_dataset[0], torch.Tensor):\n",
    "    images_tensor, labels_tensor = val_dataset\n",
    "    use_tensor_input = True\n",
    "    total = images_tensor.size(0)\n",
    "else:\n",
    "    use_tensor_input = False\n",
    "    total = len(val_dataset)\n",
    "\n",
    "# --- Create a random collage of samples with predictions ---\n",
    "num_samples = 20\n",
    "indices = random.sample(range(total), num_samples)\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "\n",
    "for ax, idx in zip(axes.flatten(), indices):\n",
    "    # Select image and true label\n",
    "    if use_tensor_input:\n",
    "        x = images_tensor[idx].unsqueeze(0).to(device)\n",
    "        true_label = int(labels_tensor[idx].item())\n",
    "    else:\n",
    "        sample = val_dataset[idx]\n",
    "        # Support dict or tuple formats\n",
    "        img = sample.get(\"image\", sample.get(\"img\")) if isinstance(sample, dict) else sample[0]\n",
    "        true_label = sample.get(\"label\", sample.get(\"target\")) if isinstance(sample, dict) else int(sample[1])\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Forward pass through encoder + head\n",
    "    with torch.no_grad():\n",
    "        feats  = encoder(x)\n",
    "        logits = head(feats)\n",
    "        probs  = F.softmax(logits, dim=1)\n",
    "    pred = probs.argmax(dim=1).item()\n",
    "\n",
    "    # Unnormalize and plot\n",
    "    img_pil = unnormalize(x[0])\n",
    "    ax.imshow(img_pil)\n",
    "    ax.set_title(f\"T: {class_names[true_label]}\\nP: {class_names[pred]} ({probs[0,pred]:.2f})\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"collage_res18.png\", dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Collage saved as collage_res18.png\")\n",
    "\n",
    "# --- Inference over the entire validation set and feature extraction ---\n",
    "y_true, y_pred, features_list = [], [], []\n",
    "\n",
    "for idx in range(total):\n",
    "    if use_tensor_input:\n",
    "        x = images_tensor[idx].unsqueeze(0).to(device)\n",
    "        label = int(labels_tensor[idx].item())\n",
    "    else:\n",
    "        sample = val_dataset[idx]\n",
    "        if isinstance(sample, dict):\n",
    "            img = sample.get(\"image\", sample.get(\"img\"))\n",
    "            label = int(sample.get(\"label\", sample.get(\"target\", -1)))\n",
    "        else:\n",
    "            img, label = sample[0], int(sample[1])\n",
    "        x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feat   = encoder(x)\n",
    "        logits = head(feat)\n",
    "        probs  = F.softmax(logits, dim=1)\n",
    "    pred = probs.argmax(dim=1).item()\n",
    "\n",
    "    y_true.append(label)\n",
    "    y_pred.append(pred)\n",
    "    features_list.append(feat.cpu().squeeze().numpy())\n",
    "\n",
    "# --- Plot Confusion Matrix for ResNet-18 ---\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='viridis')\n",
    "plt.title(\"Confusion Matrix ResNet-18\", fontsize=16)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=14)\n",
    "plt.ylabel(\"True Label\", fontsize=14)\n",
    "plt.xticks(np.arange(len(class_names)), class_names, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(len(class_names)), class_names)\n",
    "plt.colorbar(label=\"Number of Samples\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_res18.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- t-SNE Visualization of Feature Embeddings for ResNet-18 ---\n",
    "features = np.stack(features_list)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "z = tsne.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "# Use a discrete colormap with boundary normalization\n",
    "cmap = plt.get_cmap('tab10', len(class_names))\n",
    "norm = mcolors.BoundaryNorm(boundaries=np.arange(len(class_names)+1)-0.5, ncolors=cmap.N)\n",
    "\n",
    "scatter = plt.scatter(z[:, 0], z[:, 1], c=y_true, cmap=cmap, norm=norm, s=10)\n",
    "plt.title(\"t-SNE Feature Embeddings (ResNet-18)\", fontsize=16)\n",
    "plt.xlabel(\"t-SNE Component 1\", fontsize=14)\n",
    "plt.ylabel(\"t-SNE Component 2\", fontsize=14)\n",
    "\n",
    "# Add a colorbar with class names\n",
    "cbar = plt.colorbar(scatter, ticks=np.arange(len(class_names)))\n",
    "cbar.ax.set_yticklabels(class_names)\n",
    "cbar.set_label(\"Classes\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tsne_res18.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
