{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f08c1ba-9d35-4a2a-ac69-4f7311c08f57",
   "metadata": {},
   "source": [
    "### Training Resnet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdbb223-9e2f-427c-ba4f-6bc8b0601d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 08:53:23.200522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750668803.221341   19581 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750668803.227707   19581 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750668803.244076   19581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750668803.244097   19581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750668803.244099   19581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750668803.244101   19581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-23 08:53:23.249813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu126\n"
     ]
    }
   ],
   "source": [
    "# Imports & Setup\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "from glob import iglob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch_optimizer as optim_extra\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# Print the PyTorch version to verify environment setup\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ba1e4b-3ba8-4ea4-89dd-db270869cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for loading TIFF images\n",
    "class TiffRGBDataset(Dataset):\n",
    "    \"\"\"Images in TIFF Format\"\"\"\n",
    "    def __init__(self, root_dir, max_samples=None, transform=None):\n",
    "        # Expand user path and store transform\n",
    "        root_dir = os.path.expanduser(root_dir)\n",
    "        self.transform = transform\n",
    "        # Create a pattern to find all .tif/.TIF files recursively\n",
    "        pattern = os.path.join(root_dir, '**', '*.[tT][iI][fF]')\n",
    "        # Count total files matching the pattern\n",
    "        total = sum(1 for _ in iglob(pattern, recursive=True))\n",
    "        # Optionally limit number of samples\n",
    "        self.samples = sorted(iglob(pattern, recursive=True))[:max_samples] if max_samples else sorted(iglob(pattern, recursive=True))\n",
    "        print(f\"[DEBUG] {len(self.samples)} TIFF images loaded\")  # Debug print\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return number of samples\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Open image, convert to RGB, and apply transform if provided\n",
    "        img = Image.open(self.samples[idx]).convert('RGB')\n",
    "        return self.transform(img) if self.transform else img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f911b44-8473-4f2a-98d3-39b243892eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calculate mean and std for all RGB-TIFFs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Stelle sicher, dass alle Bilder die gleiche Größe haben\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      6\u001b[0m ])\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTiffRGBDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/s2_rgb/0k_251k_uint8_jpeg_tif/rgb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Dataset laden\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# dataset = RGBPNGDataset(\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     root_dir=\"data/ssl4eo-s12-subset/rgb\",\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     max_samples=None,\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     transform=transform\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mTiffRGBDataset.__init__\u001b[0;34m(self, root_dir, max_samples, transform)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m      8\u001b[0m pattern \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.[tT][iI][fF]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(iglob(pattern, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))[:max_samples] \u001b[38;5;28;01mif\u001b[39;00m max_samples \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(iglob(pattern, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m TIFF-Bilder geladen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m      8\u001b[0m pattern \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.[tT][iI][fF]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m iglob(pattern, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(iglob(pattern, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))[:max_samples] \u001b[38;5;28;01mif\u001b[39;00m max_samples \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(iglob(pattern, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m TIFF-Bilder geladen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/glob.py:86\u001b[0m, in \u001b[0;36m_iglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, dironly)\u001b[0m\n\u001b[1;32m     84\u001b[0m     glob_in_dir \u001b[38;5;241m=\u001b[39m _glob0\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirname \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglob_in_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/glob.py:94\u001b[0m, in \u001b[0;36m_glob1\u001b[0;34m(dirname, pattern, dir_fd, dironly)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_glob1\u001b[39m(dirname, pattern, dir_fd, dironly):\n\u001b[0;32m---> 94\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[43m_listdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(pattern):\n\u001b[1;32m     96\u001b[0m         names \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(x))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/glob.py:164\u001b[0m, in \u001b[0;36m_listdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_listdir\u001b[39m(dirname, dir_fd, dironly):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(_iterdir(dirname, dir_fd, dironly)) \u001b[38;5;28;01mas\u001b[39;00m it:\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/glob.py:149\u001b[0m, in \u001b[0;36m_iterdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dironly \u001b[38;5;129;01mor\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m fsencode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m                 \u001b[38;5;28;01myield\u001b[39;00m fsencode(entry\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define transformation to resize images and convert to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    transforms.ToTensor(),           # Convert PIL Image to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Initialize the TIFF dataset with up to 100k samples\n",
    "dataset = TiffRGBDataset(\n",
    "    root_dir=\"data/s2_rgb/0k_251k_uint8_jpeg_tif/rgb\",\n",
    "    max_samples=100000,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create DataLoader for batching and loading data\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Initialize accumulators for mean and standard deviation calculation\n",
    "sum_ = torch.zeros(3)    # Sum of pixel values per channel\n",
    "sum_sq = torch.zeros(3)  # Sum of squared pixel values per channel\n",
    "cnt = 0                  # Total number of pixels processed\n",
    "\n",
    "# Iterate through the dataset in batches\n",
    "for batch in tqdm(loader, desc=\"Computing mean and std\"):  # Progress bar for monitoring\n",
    "    batch = batch.to(torch.float32)  # Ensure data is float32 for numerical stability\n",
    "    b, c, h, w = batch.shape          # Batch size, channels, height, width\n",
    "    # Accumulate sum of pixels and sum of squared pixels\n",
    "    sum_ += batch.sum(dim=[0, 2, 3])         # Sum across batch, height, and width dims\n",
    "    sum_sq += (batch ** 2).sum(dim=[0, 2, 3])  # Sum of squares for each channel\n",
    "    cnt += b * h * w                           # Increment pixel count\n",
    "\n",
    "# Compute mean and standard deviation for each channel\n",
    "mean = sum_ / cnt\n",
    "std = torch.sqrt(sum_sq / cnt - mean ** 2)\n",
    "\n",
    "# Print the computed statistics\n",
    "print(f\"Mean: {mean.tolist()}\")\n",
    "print(f\"Std:  {std.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0456333c-0c13-4b6d-aa33-bfe8c7581564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation in for Barlow Twins\n",
    "#Mean for subset RGB tensor([0.3601, 0.3573, 0.3337]) \n",
    "#STD for subset RGB tensor([0.2403, 0.2317, 0.2347])\n",
    "#Mean: for 100k image set [0.4824180603027344, 0.4808058738708496, 0.47794070839881897]\n",
    "# Std: for 100k image set [0.19021621346473694, 0.16879530251026154, 0.14623168110847473]\n",
    "\n",
    "\n",
    "# Aus Deiner Ausgabe kopiert\n",
    "mean_list = [0.4824180603027344, 0.4808058738708496, 0.47794070839881897]\n",
    "std_list  = [0.19021621346473694, 0.16879530251026154, 0.14623168110847473]\n",
    "\n",
    "# In Torch-Tensoren umwandeln\n",
    "mean = torch.tensor(mean_list, dtype=torch.float32)\n",
    "std  = torch.tensor(std_list,  dtype=torch.float32)\n",
    "\n",
    "base = [\n",
    "    T.RandomResizedCrop(224, scale=(0.08,1.0), interpolation=InterpolationMode.BICUBIC),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomApply([T.ColorJitter(0.8,0.8,0.8,0.2)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=int(0.1*224)|1)], p=0.5),\n",
    "]\n",
    "\n",
    "transform_1 = T.Compose(base + [\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "transform_2 = T.Compose(base + [\n",
    "    T.RandomSolarize(threshold=0.5, p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# TwoCropTransform for Barlow Twins\n",
    "class TwoCropTransformBT:\n",
    "    def __init__(self, t1, t2):\n",
    "        self.t1 = t1\n",
    "        self.t2 = t2\n",
    "    def __call__(self, img):\n",
    "        return self.t1(img), self.t2(img)\n",
    "\n",
    "# Augmentation-Schritte visualisieren\n",
    "def log_augment_steps(img: Image.Image, writer: SummaryWriter, base_transforms: list, step: int = 0):\n",
    "    \"\"\"\n",
    "    Zeigt jeweils das Ergebnis nach jedem Basisschritt.\n",
    "    \"\"\"\n",
    "    ops = [\n",
    "        (\"01_ResizeCrop\", base_transforms[0]),\n",
    "        (\"02_HFlip\",       base_transforms[1]),\n",
    "        (\"03_ColorJitter\", base_transforms[2].transforms[0] if isinstance(base_transforms[2], T.RandomApply) else base_transforms[2]),\n",
    "        (\"04_Gray\",        base_transforms[3]),\n",
    "        (\"05_GaussianBlur\", base_transforms[4].transforms[0] if isinstance(base_transforms[4], T.RandomApply) else base_transforms[4]),\n",
    "    ]\n",
    "    x = img\n",
    "    for name, op in ops:\n",
    "        x = op(x)\n",
    "        t = T.ToTensor()(x)\n",
    "        writer.add_image(f\"Augment/{name}\", torchvision.utils.make_grid(t.unsqueeze(0), normalize=True), step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122feed5-df2b-4faa-802d-58c058b2e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Erstellen des HDF5-Datensatzes\n",
    "\n",
    "def prepare_h5(root_dir, out_path, max_samples, log_dir):\n",
    "    writer = SummaryWriter(log_dir=os.path.join(log_dir, \"prep\"))\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "\n",
    "    dataset = TiffRGBDataset(\n",
    "        root_dir=root_dir,\n",
    "        transform=TwoCropTransformBT(transform_1, transform_2),\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    \n",
    "    writer.add_text(\"Dataset/Info\",\n",
    "                    f\"Root: {root_dir}\\nSamples: {len(dataset)}\\nTransforms: {base}\",\n",
    "                    global_step=0)\n",
    "\n",
    "    # Original-Sample loggen\n",
    "    sample_orig = Image.open(dataset.samples[min(1, len(dataset)-1)]).convert(\"RGB\")\n",
    "    orig_t = T.ToTensor()(sample_orig)\n",
    "    writer.add_image(\"Dataset/OriginalSample\",\n",
    "                     torchvision.utils.make_grid(orig_t.unsqueeze(0), normalize=True),\n",
    "                     global_step=0)\n",
    "\n",
    "    log_augment_steps(sample_orig, writer, base, step=0)\n",
    "\n",
    "    # Histogramm der Crop-Skalen\n",
    "    scales = []\n",
    "    for _ in range(100):\n",
    "        _, _, h, _ = T.RandomResizedCrop.get_params(sample_orig, scale=(0.8,1.0), ratio=(1,1))\n",
    "        scales.append(h/224)\n",
    "    writer.add_histogram(\"Augment/ScaleDist\", torch.tensor(scales), global_step=0)\n",
    "\n",
    "    # HDF5 schreiben\n",
    "    N = len(dataset)\n",
    "    with h5py.File(out_path, \"w\") as f:\n",
    "        d1 = f.create_dataset(\"view1\", (N, 3, 224, 224), dtype=\"uint8\")\n",
    "        d2 = f.create_dataset(\"view2\", (N, 3, 224, 224), dtype=\"uint8\")\n",
    "        \n",
    "        for i in trange(N, desc=\"Schreibe HDF5\"):\n",
    "            x1, x2 = dataset[i]\n",
    "            d1[i] = (x1.mul(255).byte().numpy())  # Skaliere zu [0, 255] und speichere als Byte\n",
    "            d2[i] = (x2.mul(255).byte().numpy())  # Für beide Ansichten (view1 und view2)\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"HDF5 gespeichert: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfbca7a-e691-4617-ab31-8998590d3b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell, Loss-Funktion & Trainings-Loop\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.f = h5py.File(path, \"r\")\n",
    "        self.v1, self.v2 = self.f[\"view1\"], self.f[\"view2\"]\n",
    "    def __len__(self): return self.v1.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        # Gibt das Bildpaar als Tensor zurück\n",
    "        i1 = torch.from_numpy(self.v1[idx].astype(np.float32) / 255.)  # Normalisieren auf [0,1]\n",
    "        i2 = torch.from_numpy(self.v2[idx].astype(np.float32) / 255.)       \n",
    "        return i1, i2\n",
    "\n",
    "class BarlowTwinsModel(nn.Module):\n",
    "    def __init__(self, proj_dim=2048, hidden_dim=8192):\n",
    "        super().__init__()\n",
    "        # 1) Pretrained ResNet-50 als Encoder\n",
    "        self.backbone = resnet50(pretrained=True)\n",
    "        feat_dim = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # 2) Projector mit 3 Schichten\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(feat_dim,   hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, proj_dim,   bias=False),\n",
    "            nn.BatchNorm1d(proj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        return self.projector(self.backbone(x1)), self.projector(self.backbone(x2))\n",
    "\n",
    "def off_diagonal(x):\n",
    "    n,_ = x.shape\n",
    "    return x.flatten()[:-1].view(n-1,n+1)[:,1:].flatten()\n",
    "\n",
    "def barlow_twins_loss(z1, z2, lambda_offdiag=5e-3):\n",
    "    # z1, z2: [B, D]\n",
    "    B, D = z1.size()\n",
    "    # 1) Standardisierung\n",
    "    z1 = (z1 - z1.mean(0)) / z1.std(0)\n",
    "    z2 = (z2 - z2.mean(0)) / z2.std(0)\n",
    "    # 2) Korrelationsmatrix\n",
    "    C = (z1.T @ z2) / B   # [D, D]\n",
    "    # 3) Loss\n",
    "    diag_loss    = torch.sum((torch.diagonal(C) - 1) ** 2)\n",
    "    offdiag_loss = torch.sum(C**2) - torch.sum(torch.diagonal(C)**2)\n",
    "    return diag_loss + lambda_offdiag * offdiag_loss\n",
    "    \n",
    "\n",
    "def train(h5_path, log_dir, total_epochs=100, batch_size=32, accum_steps=4, lr=5e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "    \n",
    "    # Dataset laden\n",
    "    ds = HDF5Dataset(h5_path)\n",
    "    print(f\"Anzahl der Samples im Dataset: {len(ds)}\")  # Debugging-Ausgabe\n",
    "\n",
    "    # Überprüfen, ob DataLoader korrekt initialisiert wurde\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True,\n",
    "                        num_workers=8, pin_memory=True,\n",
    "                        persistent_workers=True, prefetch_factor=2, drop_last=True)\n",
    "    \n",
    "    # Wenn der DataLoader leer ist, die Funktion verlassen\n",
    "    if len(loader) == 0:\n",
    "        print(\"Der DataLoader enthält keine Daten. Training kann nicht fortgesetzt werden.\")\n",
    "        return\n",
    "\n",
    "    model = BarlowTwinsModel().to(device)\n",
    "\n",
    "    # LARS-Optimizer\n",
    "    opt = optim_extra.LARS(\n",
    "        model.parameters(),\n",
    "        lr=lr,       # Basis-LR skaliert mit Batch-Size / 256\n",
    "        weight_decay=1e-6,\n",
    "        momentum=0.9,\n",
    "    )\n",
    "    \n",
    "    # Warmup und Cosine Learning Rate Scheduler\n",
    "    warmup_epochs = 10\n",
    "    \n",
    "    sched = torch.optim.lr_scheduler.SequentialLR(\n",
    "        opt,\n",
    "        schedulers=[\n",
    "            LinearLR(opt, start_factor=0.01, total_iters=warmup_epochs),\n",
    "            CosineAnnealingLR(opt, T_max=total_epochs-warmup_epochs)\n",
    "        ],\n",
    "        milestones=[warmup_epochs]\n",
    "    )\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    writer = SummaryWriter(log_dir=os.path.join(log_dir, \"train\"))\n",
    "    \n",
    "    # Checkpoint-Verzeichnis anlegen\n",
    "    ckpt_dir = os.path.join(log_dir, \"checkpoints\")\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        t0, loss_acc = time.time(), 0\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        for batch_idx, (x1, x2) in enumerate(loader):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            with autocast():\n",
    "                z1, z2 = model(x1, x2)\n",
    "                loss = barlow_twins_loss(z1, z2) / accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            loss_acc += loss.item() * accum_steps\n",
    "\n",
    "            if (batch_idx + 1) % accum_steps == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            step = epoch * len(loader) + batch_idx\n",
    "            writer.add_scalar(\"Loss/train_batch\", loss.item() * accum_steps, step)\n",
    "            writer.add_scalar(\"LR\", opt.param_groups[0]['lr'], step)\n",
    "\n",
    "        sched.step()\n",
    "        epoch_loss = loss_acc / len(loader) if len(loader) > 0 else 0\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Time/epoch\", time.time() - t0, epoch)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == total_epochs:\n",
    "            ckpt_path = os.path.join(ckpt_dir, f\"barlow_epoch{epoch+1:03d}.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "            }, ckpt_path)\n",
    "            print(f\"[CHECKPOINT] {ckpt_path} gespeichert\")\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3393ce1-dde1-45d9-a70a-fbbb34041f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-Parameter for subset\n",
    "\n",
    "# data preprocessing and saving in HDF\n",
    "mode = \"prepare\"            \n",
    "root_dir    = \"data/ssl4eo-s12-subset/rgb\"\n",
    "h5_path     = \"data/ssl4eo-s12-subset_res50/augmented_dataset_subset1.h5\"\n",
    "logdir      = \"runs/ssl4eo-s12-subset_res50\"\n",
    "max_samples = 100\n",
    "epochs      = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4a3803e-e15c-4dca-a41b-50839355b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-Parameter for subset\n",
    "\n",
    "# training\n",
    "mode = \"train\"            \n",
    "root_dir    = \"data/ssl4eo-s12-subset/rgb\"\n",
    "h5_path     = \"data/ssl4eo-s12-subset_res50/augmented_dataset_subset1.h5\"\n",
    "logdir      = \"runs/ssl4eo-s12-subset_res50\"\n",
    "max_samples = 100\n",
    "epochs= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22b38204-1c41-473b-ab47-ab3a7744f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-Parameter for total RGB\n",
    "\n",
    "# data preprocessing and saving in HDFmode\n",
    "\n",
    "mode = \"prepare\"            \n",
    "root_dir    = \"data/s2_rgb/0k_251k_uint8_jpeg_tif/rgb\"\n",
    "h5_path     = \"data/s2_rgb/augmented_dataset_100000_res50.h5\"\n",
    "logdir      = \"runs/barlow_twins_ssl4eo_rgb_100000_res50\"\n",
    "max_samples = 100000\n",
    "epochs      = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2715e98-6d63-4ac1-8953-fd35281f505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook-Parameter for total RGB\n",
    "\n",
    "# training\n",
    "\n",
    "mode = \"train\"            \n",
    "root_dir    = \"data/s2_rgb/0k_251k_uint8_jpeg_tif/rgb\"\n",
    "h5_path     = \"data/s2_rgb/augmented_dataset_100000_res50.h5\"\n",
    "logdir      = \"runs/barlow_twins_ssl4eo_rgb_100000_res50\"\n",
    "max_samples = 100000\n",
    "epochs      = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544f0052-0281-485d-b919-5c6182ae128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Anzahl der Samples im Dataset: 100000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'resnet50' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     prepare_h5(root_dir, h5_path, max_samples, logdir)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnbekannter Modus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 73\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(h5_path, log_dir, total_epochs, batch_size, accum_steps, lr)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDer DataLoader enthält keine Daten. Training kann nicht fortgesetzt werden.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBarlowTwinsModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# LARS-Optimizer\u001b[39;00m\n\u001b[1;32m     76\u001b[0m opt \u001b[38;5;241m=\u001b[39m optim_extra\u001b[38;5;241m.\u001b[39mLARS(\n\u001b[1;32m     77\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m     78\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,       \u001b[38;5;66;03m# Basis-LR skaliert mit Batch-Size / 256\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m,\n\u001b[1;32m     80\u001b[0m     momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     81\u001b[0m )\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mBarlowTwinsModel.__init__\u001b[0;34m(self, proj_dim, hidden_dim)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 1) Pretrained ResNet-50 als Encoder\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m \u001b[43mresnet50\u001b[49m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m feat_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resnet50' is not defined"
     ]
    }
   ],
   "source": [
    "# Ausführung\n",
    "\n",
    "if mode == \"prepare\":\n",
    "    prepare_h5(root_dir, h5_path, max_samples, logdir)\n",
    "elif mode == \"train\":\n",
    "    train(h5_path, logdir, total_epochs=epochs)\n",
    "else:\n",
    "    raise ValueError(f\"Unbekannter Modus: {mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260f46a-cf3d-4d27-a4ca-c660be66eb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
